{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs('checkpoints/', exist_ok = True)\n",
    "checkpoint_path = 'checkpoints/weights.{epoch:02d}.hdf5'\n",
    "data = pd.read_csv('coin_Bitcoin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model (data, target, drop = [], thresholds = [],\n",
    "        input_node = { 'units': 80, 'activation': 'relu' },\n",
    "        hidden_nodes = [{ 'units': 30, 'activation': 'relu' }],\n",
    "        output_node = { 'units': 1, 'activation': 'sigmoid' },\n",
    "        optimizer = 'adam',\n",
    "        epochs = 100, save_path = '', seed = 0, verbose = 0):\n",
    "    \"\"\"\n",
    "    Processes data, creates a neuro network, trains, and tests it.  Returns the trained model.\n",
    "    data: target and features values in a data frame\n",
    "    target: the name of the y col in data\n",
    "    drop: cols to drop from the data\n",
    "    thresholds: an array of obects with properties 'col' and 'threshold'\n",
    "        col is the column to bin\n",
    "        threshold is teh value below which all vals should be binned into other\n",
    "    target: the name of the target col\n",
    "    input_node: an object with properties 'units' and 'activation'\n",
    "    hidden_nodes: an array of objects with properties 'units' and 'activation'\n",
    "    output_node: an object with properties 'units' and 'activation'\n",
    "        units is an int\n",
    "        activation is a string in: relu, leaky_relu, tanh, sigmoid\n",
    "    optimizer: a string in: adadelta, adagrad, adam, adamax, ftrl, optimiser, rmsprop, sgd\n",
    "    epochs: how long to train the model\n",
    "    save_path: where to save epoch weights; an empty string tells the code to not save epochs \n",
    "    seed: an int to seed the train/test split on\n",
    "    verbose: an int to set how much data the model outputs while running\n",
    "    \"\"\"\n",
    "    \n",
    "    # clone data\n",
    "    data_clone = data.copy()\n",
    "    \n",
    "    # clean data\n",
    "    data_clone = clean_data(data_clone, drop, thresholds)\n",
    "    \n",
    "    #get train/test data and make model\n",
    "    X_train, X_test, y_train, y_test = get_target_and_features(data_clone, target, seed)\n",
    "    model = create_model(X_train, input_node, hidden_nodes, output_node, optimizer)\n",
    "    \n",
    "    #run and evaluate model; return model\n",
    "    return evaluate(model, X_train, X_test, y_train, y_test, epochs, save_path, verbose)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, drop, thresholds):\n",
    "    # drop columns\n",
    "    data = data.drop(drop, 1)\n",
    "    \n",
    "    # compress large sets of data\n",
    "    for threshold in thresholds:\n",
    "        data = compress(data, threshold['col'], threshold['threshold'])\n",
    "        \n",
    "    ####TODO\n",
    "    # We need code to handle the date data appropriately\n",
    "    \n",
    "    #replace cats as dummys\n",
    "    return encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress (data, col, threshold):\n",
    "    \n",
    "    # get vals to replace\n",
    "    counts = data[col].value_counts()\n",
    "    replace = list(counts[counts < threshold].index)\n",
    "    \n",
    "    #replace the values and return\n",
    "    for rep in replace:\n",
    "        data[col] = data[col].replace(rep, 'Other')\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data):    \n",
    "    # get cats     \n",
    "    cat = data.dtypes[data.dtypes == 'object'].index.tolist()\n",
    "    \n",
    "    # make encoder and encode\n",
    "    enc = OneHotEncoder(sparse = False)\n",
    "    encode = pd.DataFrame(enc.fit_transform(data[cat]))\n",
    "    encode.columns = enc.get_feature_names(cat)    \n",
    "    \n",
    "    # return new df\n",
    "    return data.merge(encode, left_index = True, right_index = True).drop(cat, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_and_features(data, target, seed):\n",
    "    # Split our preprocessed data into our features and target arrays\n",
    "    y = data[target].values\n",
    "    X = data.drop([target], 1).values\n",
    "\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = seed)\n",
    "\n",
    "    # Create a StandardScaler instances\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the StandardScaler\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train, input_node, hidden_nodes, output_node, optimizer):\n",
    "\n",
    "    # get the input node number\n",
    "    num_features = len(X_train[0])\n",
    "    \n",
    "    # create model\n",
    "    nn = tf.keras.models.Sequential()\n",
    "    \n",
    "    #add layers\n",
    "    #input\n",
    "    nn.add(tf.keras.layers.Dense(units = input_node['units'], input_dim = num_features,\n",
    "                                 activation = input_node['activation']))\n",
    "\n",
    "    #hidden\n",
    "    for node in hidden_nodes:\n",
    "        nn.add(tf.keras.layers.Dense(units = node['units'], activation = node['activation']))\n",
    "        \n",
    "    #output\n",
    "    nn.add(tf.keras.layers.Dense(units = output_node['units'], activation = output_node['activation']))\n",
    "\n",
    "    #compile and return\n",
    "    nn.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callback (path, verbose):\n",
    "    return ModelCheckpoint(\n",
    "        filepath = path,\n",
    "        verbose = verbose,\n",
    "        save_weights_only = True,\n",
    "        save_freq = 'epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, X_test, y_train, y_test, epochs, save_path, verbose):\n",
    "    # it either wiith callpaths to save epochs or without\n",
    "    if save_path != '':\n",
    "        fit_model = model.fit(X_train, y_train, epochs = epochs, verbose = verbose,\n",
    "                              callbacks = [create_callback(save_path, verbose)])\n",
    "    else:\n",
    "        fit_model = model.fit(X_train, y_train, epochs = epochs, verbose = verbose)\n",
    "    \n",
    "    #give feedback on model performance\n",
    "    model_loss, model_accuracy = model.evaluate(X_test, y_test, verbose = verbose)\n",
    "    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "    \n",
    "    #return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -1.0603837197951959e+18, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model = run_model(data, 'Marketcap', drop = ['Name', 'Symbol', 'SNo', 'Date'], save_path = checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
